from hddu.utils import SplitPDFFilesNode
from hddu.parser import (
    DocumentParseNode,
    PostDocumentParseNode,
    WorkingQueueNode,
    SaveStateNode,
    continue_parse,
)

from hddu.logging_config import get_logger, setup_verbose_logging
import time
import os

logger = get_logger(__name__)

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    logger.debug("psutil ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

from langgraph.graph import StateGraph, END
from hddu.state import ParseState
from langchain_utils.graphs import visualize_graph
from langgraph.checkpoint.memory import MemorySaver

from hddu.preprocessing import (
    SaveImageNode,
    RefineContentNode,
    CreateElementsNode,
    MergeEntityNode,
    ReconstructElementsNode,
    GenerateComprehensiveMarkdownNode,
    LangChainDocumentNode,
    SaveFinalStateNode,
)

from hddu.translate import add_translation_module
from hddu.interpreter import contextualize_text
from hddu.extractor import (
    PageElementsExtractorNode,
    ImageEntityExtractorNode,
    TableEntityExtractorNode,
)


def create_complete_workflow(batch_size=1, test_page=None, verbose=True):

    if verbose:
        logger.info("ì›Œí¬í”Œë¡œìš° êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹œì‘...")
    
    parser_start = time.time()
    
    split_pdf_node = SplitPDFFilesNode(
        batch_size=batch_size, test_page=test_page, verbose=verbose
    )
    
    document_parse_node = DocumentParseNode(
        lang="auto", verbose=verbose
    )
    
    post_document_parse_node = PostDocumentParseNode(verbose=verbose)
    
    working_queue_node = WorkingQueueNode(verbose=verbose)
    
    parser_save_state_node = SaveStateNode(verbose=verbose)
    
    parser_time = time.time() - parser_start
    
    if verbose:
        logger.info(f"Document Parser ë…¸ë“œë“¤ ìƒì„± ì™„ë£Œ ({parser_time:.2f}ì´ˆ)")
        logger.info(f"ì„¤ì •: batch_size={batch_size}, test_page={test_page}")
    
    nodes_start = time.time()
    
    if verbose:
        logger.debug("ì›Œí¬í”Œë¡œìš° ë…¸ë“œë“¤ ìƒì„± ì¤‘...")
    
    save_image_node = SaveImageNode(verbose=verbose)
    refine_content_node = RefineContentNode(verbose=verbose)
    create_elements_node = CreateElementsNode(verbose=verbose)
    page_elements_extractor_node = PageElementsExtractorNode(verbose=verbose)
    image_entity_extractor_node = ImageEntityExtractorNode(verbose=verbose)
    table_entity_extractor_node = TableEntityExtractorNode(verbose=verbose)
    merge_entity_node = MergeEntityNode(verbose=verbose)
    reconstruct_elements_node = ReconstructElementsNode(verbose=verbose)
    generate_comprehensive_markdown_node = GenerateComprehensiveMarkdownNode(verbose=verbose)
    langchain_document_node = LangChainDocumentNode(verbose=verbose)
    save_final_state_node = SaveFinalStateNode(verbose=verbose)
    
    nodes_time = time.time() - nodes_start
    
    if verbose:
        logger.info(f"16ê°œ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ìƒì„± ì™„ë£Œ ({nodes_time:.2f}ì´ˆ)")

    graph_start = time.time()
    parent_workflow = StateGraph(ParseState)

    if verbose:
        logger.debug("ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ì— ë…¸ë“œë“¤ ì¶”ê°€ ì¤‘...")

    node_names = [
        ("split_pdf_node", split_pdf_node, "PDF ë¶„í• "),
        ("document_parse_node", document_parse_node, "ë¬¸ì„œ íŒŒì‹±"),
        ("post_document_parse_node", post_document_parse_node, "íŒŒì‹± í›„ì²˜ë¦¬"),
        ("working_queue_node", working_queue_node, "ì‘ì—… í ê´€ë¦¬"),
        ("parser_save_state_node", parser_save_state_node, "íŒŒì‹± ìƒíƒœ ì €ì¥"),
        ("save_image_node", save_image_node, "ì´ë¯¸ì§€ ì €ì¥"),
        ("refine_content_node", refine_content_node, "í…ìŠ¤íŠ¸ ì •ì œ"),
        ("add_translation", add_translation_module, "ë²ˆì—­"),
        ("contextualize_text", contextualize_text, "ë¬¸ë§¥í™”"),
        ("create_elements_node", create_elements_node, "ìš”ì†Œ ìƒì„±"),
        ("page_elements_extractor", page_elements_extractor_node, "í˜ì´ì§€ë³„ ìš”ì†Œ ì¶”ì¶œ"),
        ("image_entity_extractor", image_entity_extractor_node, "ì´ë¯¸ì§€ ì—”í‹°í‹° ì¶”ì¶œ"),
        ("table_entity_extractor", table_entity_extractor_node, "í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ"),
        ("merge_entity_node", merge_entity_node, "ì—”í‹°í‹° ë³‘í•©"),
        ("reconstruct_elements_node", reconstruct_elements_node, "ìš”ì†Œ ì¬êµ¬ì„±"),
        ("generate_comprehensive_markdown", generate_comprehensive_markdown_node, "ë§ˆí¬ë‹¤ìš´ ìƒì„±"),
        ("langchain_document_node", langchain_document_node, "LangChain ë¬¸ì„œ ìƒì„±"),
        ("save_final_state", save_final_state_node, "ìµœì¢… ìƒíƒœ ì €ì¥")
    ]
    
    for node_name, node_instance, description in node_names:
        parent_workflow.add_node(node_name, node_instance)
        if verbose:
            logger.debug(f"  ë…¸ë“œ ì¶”ê°€: {node_name} ({description})")
    
    if verbose:
        logger.info(f"{len(node_names)}ê°œ ë…¸ë“œê°€ ì›Œí¬í”Œë¡œìš°ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    if verbose:
        logger.debug("ë…¸ë“œ ê°„ ì—°ê²°(ì—£ì§€) ì„¤ì • ì¤‘...")
    
    parent_workflow.add_edge("split_pdf_node", "working_queue_node")
    parent_workflow.add_conditional_edges(
        "working_queue_node",
        continue_parse,
        {True: "document_parse_node", False: "post_document_parse_node"},
    )
    parent_workflow.add_edge("document_parse_node", "working_queue_node")
    parent_workflow.add_edge("post_document_parse_node", "parser_save_state_node")
    
    if verbose:
        logger.debug("  íŒŒì‹± ë‹¨ê³„ ì—°ê²° ì™„ë£Œ (ì¡°ê±´ë¶€ ë£¨í”„ í¬í•¨)")
    
    parent_workflow.add_edge("parser_save_state_node", "save_image_node")
    
    sequential_edges = [
        ("save_image_node", "refine_content_node", "ì´ë¯¸ì§€ ì €ì¥ -> í…ìŠ¤íŠ¸ ì •ì œ"),
        ("refine_content_node", "add_translation", "í…ìŠ¤íŠ¸ ì •ì œ -> ë²ˆì—­"),
        ("add_translation", "contextualize_text", "ë²ˆì—­ -> ë¬¸ë§¥í™”"),
        ("contextualize_text", "create_elements_node", "ë¬¸ë§¥í™” -> ìš”ì†Œ ìƒì„±"),
        ("create_elements_node", "page_elements_extractor", "ìš”ì†Œ ìƒì„± -> í˜ì´ì§€ë³„ ì¶”ì¶œ")
    ]
    
    for from_node, to_node, description in sequential_edges:
        parent_workflow.add_edge(from_node, to_node)
        if verbose:
            logger.debug(f"  ìˆœì°¨ ì—°ê²°: {description}")
    
    if verbose:
        logger.debug("  ë³‘ë ¬ ì²˜ë¦¬ êµ¬ê°„ ì„¤ì •:")
        logger.debug("    í˜ì´ì§€ë³„ ì¶”ì¶œ -> ì´ë¯¸ì§€/í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ (ë³‘ë ¬)")
    
    parent_workflow.add_edge("page_elements_extractor", "image_entity_extractor")
    parent_workflow.add_edge("page_elements_extractor", "table_entity_extractor")
    
    parent_workflow.add_edge("image_entity_extractor", "merge_entity_node")
    parent_workflow.add_edge("table_entity_extractor", "merge_entity_node")
    
    if verbose:
        logger.debug("    ì´ë¯¸ì§€/í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ -> ì—”í‹°í‹° ë³‘í•©")
    
    parent_workflow.add_edge("merge_entity_node", "reconstruct_elements_node")
    
    if verbose:
        logger.debug("    ìš”ì†Œ ì¬êµ¬ì„± -> ë§ˆí¬ë‹¤ìš´/ë¬¸ì„œ ìƒì„± (ë³‘ë ¬)")
    
    parent_workflow.add_edge("reconstruct_elements_node", "generate_comprehensive_markdown")
    parent_workflow.add_edge("reconstruct_elements_node", "langchain_document_node")
    
    parent_workflow.add_edge("generate_comprehensive_markdown", "save_final_state")
    
    if verbose:
        logger.debug("    ë§ˆí¬ë‹¤ìš´ ìƒì„± -> ìµœì¢… ìƒíƒœ ì €ì¥")
        logger.debug("    ë¬¸ì„œ ìƒì„±ì€ ë³‘ë ¬ë¡œ ì™„ë£Œ")

    parent_workflow.set_entry_point("split_pdf_node")
    
    if verbose:
        logger.debug("ì›Œí¬í”Œë¡œìš° ì‹œì‘ì  ì„¤ì •: split_pdf_node")

    compile_start = time.time()
    parent_graph = parent_workflow.compile(checkpointer=MemorySaver())
    compile_time = time.time() - compile_start
    
    graph_total_time = time.time() - graph_start
    
    if verbose:
        logger.info(f"ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ ({compile_time:.2f}ì´ˆ)")
        logger.info(f"ì „ì²´ ê·¸ë˜í”„ êµ¬ì„± ì‹œê°„: {graph_total_time:.2f}ì´ˆ")
    
    if verbose:
        setup_verbose_logging(verbose)
        
        total_creation_time = parser_time + nodes_time + graph_total_time
        logger.info("=" * 50)
        logger.info("ì™„ì „í•œ í†µí•© ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
        logger.info("=" * 50)
        logger.info(f"ì´ ìƒì„± ì‹œê°„: {total_creation_time:.2f}ì´ˆ")
        logger.info(f"  - Parser ë…¸ë“œë“¤: {parser_time:.2f}ì´ˆ")
        logger.info(f"  - ê¸°íƒ€ ë…¸ë“œë“¤: {nodes_time:.2f}ì´ˆ")
        logger.info(f"  - ê·¸ë˜í”„ êµ¬ì„±: {graph_total_time:.2f}ì´ˆ")
        
        logger.info(f"ë…¸ë“œ êµ¬ì„±: {len(node_names)}ê°œ ë…¸ë“œ")
        logger.info("í†µí•© ì›Œí¬í”Œë¡œìš° êµ¬ì¡°:")
        logger.info("  ë‹¨ê³„ 1: PDF íŒŒì‹± (ì¡°ê±´ë¶€ ë£¨í”„)")
        logger.info("    â†’ PDF ë¶„í•  â†’ ì‘ì—…í â†” ë¬¸ì„œíŒŒì‹± â†’ í›„ì²˜ë¦¬ â†’ ìƒíƒœì €ì¥")
        logger.info("  ë‹¨ê³„ 2: ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì²˜ë¦¬")
        logger.info("    â†’ ì´ë¯¸ì§€ ì €ì¥ â†’ í…ìŠ¤íŠ¸ ì •ì œ")
        logger.info("  ë‹¨ê³„ 3: ì–¸ì–´ ì²˜ë¦¬")
        logger.info("    â†’ ë²ˆì—­ â†’ ë¬¸ë§¥í™” â†’ ìš”ì†Œ ìƒì„±")
        logger.info("  ë‹¨ê³„ 4: ì—”í‹°í‹° ì¶”ì¶œ (ë³‘ë ¬)")
        logger.info("    â†’ í˜ì´ì§€ë³„ ì¶”ì¶œ â”¬â†’ ì´ë¯¸ì§€ ì—”í‹°í‹°")
        logger.info("                    â””â†’ í…Œì´ë¸” ì—”í‹°í‹°")
        logger.info("  ë‹¨ê³„ 5: ìµœì¢… ì²˜ë¦¬ (ë³‘ë ¬)")
        logger.info("    â†’ ì—”í‹°í‹° ë³‘í•© â†’ ìš”ì†Œ ì¬êµ¬ì„± â”¬â†’ ë§ˆí¬ë‹¤ìš´ ìƒì„±")
        logger.info("                              â””â†’ ë¬¸ì„œ ìƒì„±")
        logger.info("  ë‹¨ê³„ 6: ì €ì¥")
        logger.info("    â†’ ìµœì¢… ìƒíƒœ ì €ì¥")
        
        logger.info("ë³‘ë ¬ ì²˜ë¦¬ êµ¬ê°„: 2ê°œ")
        logger.info("  - ì´ë¯¸ì§€/í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ")
        logger.info("  - ë§ˆí¬ë‹¤ìš´/ë¬¸ì„œ ìƒì„±")
        logger.info("ğŸ”§ SubGraph ì œê±°: metadata/raw_elements ì¤‘ë³µ í•´ê²°")
        logger.info("=" * 50)

    return parent_graph


def create_workflow_from_assembled(verbose=True):

    
    if verbose:
        logger.info("Assembled ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì‹œì‘...")
    
    nodes_start = time.time()
    
    save_image_node = SaveImageNode(verbose=verbose)
    refine_content_node = RefineContentNode(verbose=verbose)
    create_elements_node = CreateElementsNode(verbose=verbose)
    page_elements_extractor_node = PageElementsExtractorNode(verbose=verbose)
    image_entity_extractor_node = ImageEntityExtractorNode(verbose=verbose)
    table_entity_extractor_node = TableEntityExtractorNode(verbose=verbose)
    merge_entity_node = MergeEntityNode(verbose=verbose)
    reconstruct_elements_node = ReconstructElementsNode(verbose=verbose)
    generate_comprehensive_markdown_node = GenerateComprehensiveMarkdownNode(verbose=verbose)
    langchain_document_node = LangChainDocumentNode(verbose=verbose)
    save_final_state_node = SaveFinalStateNode(verbose=verbose)
    
    nodes_time = time.time() - nodes_start
    
    if verbose:
        logger.info(f"11ê°œ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ìƒì„± ì™„ë£Œ ({nodes_time:.2f}ì´ˆ)")
    
    graph_start = time.time()
    parent_workflow = StateGraph(ParseState)
    
    if verbose:
        logger.debug("ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ì— ë…¸ë“œë“¤ ì¶”ê°€ ì¤‘...")
    
    node_names = [
        ("save_image_node", save_image_node, "ì´ë¯¸ì§€ ì €ì¥"),
        ("refine_content_node", refine_content_node, "í…ìŠ¤íŠ¸ ì •ì œ"),
        ("add_translation", add_translation_module, "ë²ˆì—­"),
        ("contextualize_text", contextualize_text, "ë¬¸ë§¥í™”"),
        ("create_elements_node", create_elements_node, "ìš”ì†Œ ìƒì„±"),
        ("page_elements_extractor", page_elements_extractor_node, "í˜ì´ì§€ë³„ ìš”ì†Œ ì¶”ì¶œ"),
        ("image_entity_extractor", image_entity_extractor_node, "ì´ë¯¸ì§€ ì—”í‹°í‹° ì¶”ì¶œ"),
        ("table_entity_extractor", table_entity_extractor_node, "í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ"),
        ("merge_entity_node", merge_entity_node, "ì—”í‹°í‹° ë³‘í•©"),
        ("reconstruct_elements_node", reconstruct_elements_node, "ìš”ì†Œ ì¬êµ¬ì„±"),
        ("generate_comprehensive_markdown", generate_comprehensive_markdown_node, "ë§ˆí¬ë‹¤ìš´ ìƒì„±"),
        ("langchain_document_node", langchain_document_node, "LangChain ë¬¸ì„œ ìƒì„±"),
        ("save_final_state", save_final_state_node, "ìµœì¢… ìƒíƒœ ì €ì¥")
    ]
    
    for node_name, node_instance, description in node_names:
        parent_workflow.add_node(node_name, node_instance)
        if verbose:
            logger.debug(f"  ë…¸ë“œ ì¶”ê°€: {node_name} ({description})")
    
    if verbose:
        logger.info(f"{len(node_names)}ê°œ ë…¸ë“œê°€ ì›Œí¬í”Œë¡œìš°ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    if verbose:
        logger.debug("ë…¸ë“œ ê°„ ì—°ê²°(ì—£ì§€) ì„¤ì • ì¤‘...")
    
    sequential_edges = [
        ("save_image_node", "refine_content_node", "ì´ë¯¸ì§€ ì €ì¥ -> í…ìŠ¤íŠ¸ ì •ì œ"),
        ("refine_content_node", "add_translation", "í…ìŠ¤íŠ¸ ì •ì œ -> ë²ˆì—­"),
        ("add_translation", "contextualize_text", "ë²ˆì—­ -> ë¬¸ë§¥í™”"),
        ("contextualize_text", "create_elements_node", "ë¬¸ë§¥í™” -> ìš”ì†Œ ìƒì„±"),
        ("create_elements_node", "page_elements_extractor", "ìš”ì†Œ ìƒì„± -> í˜ì´ì§€ë³„ ì¶”ì¶œ")
    ]
    
    for from_node, to_node, description in sequential_edges:
        parent_workflow.add_edge(from_node, to_node)
        if verbose:
            logger.debug(f"  ìˆœì°¨ ì—°ê²°: {description}")
    
    if verbose:
        logger.debug("  ë³‘ë ¬ ì²˜ë¦¬ êµ¬ê°„ ì„¤ì •:")
        logger.debug("    í˜ì´ì§€ë³„ ì¶”ì¶œ -> ì´ë¯¸ì§€/í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ (ë³‘ë ¬)")
    
    parent_workflow.add_edge("page_elements_extractor", "image_entity_extractor")
    parent_workflow.add_edge("page_elements_extractor", "table_entity_extractor")
    
    parent_workflow.add_edge("image_entity_extractor", "merge_entity_node")
    parent_workflow.add_edge("table_entity_extractor", "merge_entity_node")
    
    if verbose:
        logger.debug("    ì´ë¯¸ì§€/í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ -> ì—”í‹°í‹° ë³‘í•©")
    
    parent_workflow.add_edge("merge_entity_node", "reconstruct_elements_node")
    
    if verbose:
        logger.debug("    ìš”ì†Œ ì¬êµ¬ì„± -> ë§ˆí¬ë‹¤ìš´/ë¬¸ì„œ ìƒì„± (ë³‘ë ¬)")
    
    parent_workflow.add_edge("reconstruct_elements_node", "generate_comprehensive_markdown")
    parent_workflow.add_edge("reconstruct_elements_node", "langchain_document_node")
    
    parent_workflow.add_edge("generate_comprehensive_markdown", "save_final_state")
    
    if verbose:
        logger.debug("    ë§ˆí¬ë‹¤ìš´ ìƒì„± -> ìµœì¢… ìƒíƒœ ì €ì¥")
        logger.debug("    ë¬¸ì„œ ìƒì„±ì€ ë³‘ë ¬ë¡œ ì™„ë£Œ")

    parent_workflow.set_entry_point("save_image_node")
    
    if verbose:
        logger.debug("ì›Œí¬í”Œë¡œìš° ì‹œì‘ì  ì„¤ì •: save_image_node")

    compile_start = time.time()
    parent_graph = parent_workflow.compile(checkpointer=MemorySaver())
    compile_time = time.time() - compile_start
    
    graph_total_time = time.time() - graph_start
    
    if verbose:
        logger.info(f"Assembled ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ ({compile_time:.2f}ì´ˆ)")
        logger.info(f"ì „ì²´ ê·¸ë˜í”„ êµ¬ì„± ì‹œê°„: {graph_total_time:.2f}ì´ˆ")
    
    if verbose:
        setup_verbose_logging(verbose)
        
        total_creation_time = nodes_time + graph_total_time
        logger.info("=" * 50)
        logger.info("Assembled ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
        logger.info("=" * 50)
        logger.info(f"ì´ ìƒì„± ì‹œê°„: {total_creation_time:.2f}ì´ˆ")
        logger.info(f"  - ë…¸ë“œë“¤: {nodes_time:.2f}ì´ˆ")
        logger.info(f"  - ê·¸ë˜í”„ êµ¬ì„±: {graph_total_time:.2f}ì´ˆ")
        
        logger.info(f"ë…¸ë“œ êµ¬ì„±: {len(node_names)}ê°œ ë…¸ë“œ")
        logger.info("Assembled ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° êµ¬ì¡°:")
        logger.info("  ë‹¨ê³„ 1: ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì²˜ë¦¬")
        logger.info("    â†’ ì´ë¯¸ì§€ ì €ì¥ â†’ í…ìŠ¤íŠ¸ ì •ì œ")
        logger.info("  ë‹¨ê³„ 2: ì–¸ì–´ ì²˜ë¦¬")
        logger.info("    â†’ ë²ˆì—­ â†’ ë¬¸ë§¥í™” â†’ ìš”ì†Œ ìƒì„±")
        logger.info("  ë‹¨ê³„ 3: ì—”í‹°í‹° ì¶”ì¶œ (ë³‘ë ¬)")
        logger.info("    â†’ í˜ì´ì§€ë³„ ì¶”ì¶œ â”¬â†’ ì´ë¯¸ì§€ ì—”í‹°í‹°")
        logger.info("                    â””â†’ í…Œì´ë¸” ì—”í‹°í‹°")
        logger.info("  ë‹¨ê³„ 4: ìµœì¢… ì²˜ë¦¬ (ë³‘ë ¬)")
        logger.info("    â†’ ì—”í‹°í‹° ë³‘í•© â†’ ìš”ì†Œ ì¬êµ¬ì„± â”¬â†’ ë§ˆí¬ë‹¤ìš´ ìƒì„±")
        logger.info("                              â””â†’ ë¬¸ì„œ ìƒì„±")
        logger.info("  ë‹¨ê³„ 5: ì €ì¥")
        logger.info("    â†’ ìµœì¢… ìƒíƒœ ì €ì¥")
        
        logger.info("ë³‘ë ¬ ì²˜ë¦¬ êµ¬ê°„: 2ê°œ")
        logger.info("  - ì´ë¯¸ì§€/í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ")
        logger.info("  - ë§ˆí¬ë‹¤ìš´/ë¬¸ì„œ ìƒì„±")
        logger.info("ğŸ”§ PDF íŒŒì‹± ë‹¨ê³„ ì œê±°: assembled ë°ì´í„° í™œìš©")
        logger.info("=" * 50)

    return parent_graph


def load_assembled_state(assembled_path):

    import json
    
    logger.info(f"Assembled íŒŒì¼ ë¡œë“œ ì¤‘: {assembled_path}")
    
    if not os.path.exists(assembled_path):
        raise FileNotFoundError(f"Assembled íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {assembled_path}")
    
    with open(assembled_path, 'r', encoding='utf-8') as f:
        assembled_data = json.load(f)
    
    if "elements" in assembled_data:
        elements = assembled_data["elements"]
    elif "assembled_elements" in assembled_data:
        elements = assembled_data["assembled_elements"]
    else:
        raise ValueError("Assembled íŒŒì¼ì—ì„œ elements ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    base_name = os.path.splitext(os.path.basename(assembled_path))[0]
    if "_assembled" in base_name:
        original_name = base_name.split("_assembled")[0]
    else:
        original_name = base_name
    
    possible_paths = [
        f"data/{original_name}.pdf",
        f"data/{original_name}_TEST1P.pdf",
        f"{original_name}.pdf"
    ]
    
    original_filepath = None
    for path in possible_paths:
        if os.path.exists(path):
            original_filepath = path
            break
    
    if not original_filepath:
        logger.warning(f"ì›ë³¸ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¶”ì • ê²½ë¡œë“¤: {possible_paths}")
        original_filepath = f"data/{original_name}.pdf"
    
    texts_by_page = {}
    for element in elements:
        page = element.get("page", 1)
        if page not in texts_by_page:
            texts_by_page[page] = []
        
        content = element.get("content", {})
        text = content.get("text", "") or content.get("markdown", "") or ""
        if text:
            texts_by_page[page].append(text)
    
    state = {
        "filepath": original_filepath,
        "elements_from_parser": elements,
        "texts_by_page": texts_by_page,
        "assembled_source": assembled_path
    }
    
    logger.info(f"Assembled ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    logger.info(f"  - ìš”ì†Œ ìˆ˜: {len(elements)}")
    logger.info(f"  - í˜ì´ì§€ ìˆ˜: {len(texts_by_page)}")
    logger.info(f"  - ì›ë³¸ íŒŒì¼: {original_filepath}")
    
    return state


def run_workflow_from_assembled(assembled_path, verbose=True):

    start_time = time.time()
    
    if verbose:
        setup_verbose_logging(verbose)
        logger.info(f"Assembled ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘: {assembled_path}")
        
        if PSUTIL_AVAILABLE:
            try:
                memory_info = psutil.virtual_memory()
                logger.info(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory_info.total // (1024**3):.1f}GB (ì‚¬ìš©ë¥ : {memory_info.percent:.1f}%)")
                logger.info(f"CPU ì½”ì–´: {psutil.cpu_count()} (ì‚¬ìš©ë¥ : {psutil.cpu_percent(interval=1):.1f}%)")
            except Exception as e:
                logger.debug(f"ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        if os.path.exists(assembled_path):
            file_size = os.path.getsize(assembled_path) / (1024**2)
            logger.info(f"ì…ë ¥ íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
        else:
            logger.warning(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {assembled_path}")
        
        logger.info("-" * 50)
    
    try:
        data_load_start = time.time()
        initial_state = load_assembled_state(assembled_path)
        data_load_time = time.time() - data_load_start
        
        if verbose:
            logger.info(f"Assembled ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({data_load_time:.2f}ì´ˆ)")
    except Exception as e:
        logger.error(f"Assembled íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    workflow_creation_start = time.time()
    workflow = create_workflow_from_assembled(verbose=verbose)
    workflow_creation_time = time.time() - workflow_creation_start
    
    if verbose:
        logger.info(f"ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ ({workflow_creation_time:.2f}ì´ˆ)")
    
    try:
        execution_start = time.time()
        logger.info("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘...")
        
        config = {"configurable": {"thread_id": "assembled_workflow_thread"}}
        final_state = workflow.invoke(initial_state, config=config)
        
        execution_time = time.time() - execution_start
        logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ ({execution_time:.2f}ì´ˆ)")
        
        total_time = time.time() - start_time
        
        if verbose:
            logger.info("=" * 60)
            logger.info("Assembled ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            
            logger.info(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
            logger.info(f"ë°ì´í„° ë¡œë“œ: {data_load_time:.2f}ì´ˆ")
            logger.info(f"ì›Œí¬í”Œë¡œìš° ìƒì„±: {workflow_creation_time:.2f}ì´ˆ")
            logger.info(f"ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„: {execution_time:.2f}ì´ˆ")
            
            logger.info("-" * 40)
            logger.info("ì²˜ë¦¬ ê²°ê³¼ í†µê³„:")
            
            if "elements_from_parser" in final_state:
                elements = final_state["elements_from_parser"]
                logger.info(f"ì›ë³¸ ìš”ì†Œ ìˆ˜: {len(elements)}")
                
                categories = {}
                for element in elements:
                    cat = element.get("category", "unknown")
                    categories[cat] = categories.get(cat, 0) + 1
                
                for category, count in sorted(categories.items()):
                    logger.info(f"  {category}: {count}ê°œ")
            
            if "image_paths" in final_state:
                image_count = len(final_state["image_paths"])
                logger.info(f"ì €ì¥ëœ ì´ë¯¸ì§€ ìˆ˜: {image_count}")
            
            if "documents" in final_state:
                doc_count = len(final_state['documents'])
                logger.info(f"ìƒì„±ëœ LangChain ë¬¸ì„œ ìˆ˜: {doc_count}")
                
                total_chars = sum(len(doc.page_content) for doc in final_state['documents'])
                avg_chars = total_chars / doc_count if doc_count > 0 else 0
                logger.info(f"ì´ ë¬¸ì„œ ê¸¸ì´: {total_chars:,} ë¬¸ì")
                logger.info(f"í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_chars:.0f} ë¬¸ì")
            
            logger.info("-" * 40)
            logger.info("ìƒì„±ëœ íŒŒì¼ë“¤:")
            
            if "comprehensive_markdown" in final_state:
                logger.info(f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {final_state['comprehensive_markdown']}")
            
            if "documents_pickle_path" in final_state:
                logger.info(f"Documents pickle: {final_state['documents_pickle_path']}")
                
            if "final_state_json_path" in final_state:
                logger.info(f"ìµœì¢… ìƒíƒœ JSON: {final_state['final_state_json_path']}")
                logger.info(f"ìµœì¢… ìƒíƒœ pickle: {final_state['final_state_pickle_path']}")
            
            if PSUTIL_AVAILABLE:
                try:
                    final_memory = psutil.virtual_memory()
                    logger.info(f"ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {final_memory.percent:.1f}%")
                except Exception:
                    pass
                
            logger.info("=" * 60)
        
        return final_state
        
    except Exception as e:
        total_time = time.time() - start_time
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ)")
        logger.error(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ ì§€ì : {type(e).__name__}")
        
        logger.debug("ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
        
        try:
            if 'final_state' in locals():
                logger.info("ë¶€ë¶„ì  ì²˜ë¦¬ ê²°ê³¼ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
                if "elements_from_parser" in final_state:
                    logger.info(f"ì²˜ë¦¬ëœ ìš”ì†Œ ìˆ˜: {len(final_state['elements_from_parser'])}")
        except Exception:
            pass
            
        raise


def run_complete_workflow(pdf_filepath, batch_size=1, test_page=None, verbose=True):

    start_time = time.time()
    
    if verbose:
        setup_verbose_logging(verbose)
        logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘: {pdf_filepath}")
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
        if test_page is not None:
            logger.info(f"í…ŒìŠ¤íŠ¸ í˜ì´ì§€: {test_page}")
        
        if PSUTIL_AVAILABLE:
            try:
                memory_info = psutil.virtual_memory()
                logger.info(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory_info.total // (1024**3):.1f}GB (ì‚¬ìš©ë¥ : {memory_info.percent:.1f}%)")
                logger.info(f"CPU ì½”ì–´: {psutil.cpu_count()} (ì‚¬ìš©ë¥ : {psutil.cpu_percent(interval=1):.1f}%)")
            except Exception as e:
                logger.debug(f"ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        file_size = 0
        if os.path.exists(pdf_filepath):
            file_size = os.path.getsize(pdf_filepath) / (1024**2)
            logger.info(f"ì…ë ¥ íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
        else:
            logger.warning(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_filepath}")
        
        logger.info("-" * 50)
    
    workflow_creation_start = time.time()
    workflow = create_complete_workflow(
        batch_size=batch_size, 
        test_page=test_page, 
        verbose=verbose
    )
    workflow_creation_time = time.time() - workflow_creation_start
    
    if verbose:
        logger.info(f"ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ ({workflow_creation_time:.2f}ì´ˆ)")
    
    initial_state = {"filepath": pdf_filepath}
    
    try:
        execution_start = time.time()
        logger.info("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘...")
        
        config = {"configurable": {"thread_id": "test_workflow_thread"}}
        final_state = workflow.invoke(initial_state, config=config)
        
        execution_time = time.time() - execution_start
        logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ ({execution_time:.2f}ì´ˆ)")
        
        total_time = time.time() - start_time
        
        if verbose:
            logger.info("=" * 60)
            logger.info("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            
            logger.info(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
            logger.info(f"ì›Œí¬í”Œë¡œìš° ìƒì„±: {workflow_creation_time:.2f}ì´ˆ")
            logger.info(f"ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„: {execution_time:.2f}ì´ˆ")
            logger.info(f"ì²˜ë¦¬ ì†ë„: {file_size/total_time if file_size > 0 and total_time > 0 else 'N/A'} MB/ì´ˆ")
            
            logger.info("-" * 40)
            logger.info("ì²˜ë¦¬ ê²°ê³¼ í†µê³„:")
            
            if "elements_from_parser" in final_state:
                elements = final_state["elements_from_parser"]
                logger.info(f"íŒŒì‹±ëœ ìš”ì†Œ ìˆ˜: {len(elements)}")
                
                categories = {}
                for element in elements:
                    cat = element.get("category", "unknown")
                    categories[cat] = categories.get(cat, 0) + 1
                
                for category, count in sorted(categories.items()):
                    logger.info(f"  {category}: {count}ê°œ")
            
            if "image_paths" in final_state:
                image_count = len(final_state["image_paths"])
                logger.info(f"ì €ì¥ëœ ì´ë¯¸ì§€ ìˆ˜: {image_count}")
            
            if "documents" in final_state:
                doc_count = len(final_state['documents'])
                logger.info(f"ìƒì„±ëœ LangChain ë¬¸ì„œ ìˆ˜: {doc_count}")
                
                total_chars = sum(len(doc.page_content) for doc in final_state['documents'])
                avg_chars = total_chars / doc_count if doc_count > 0 else 0
                logger.info(f"ì´ ë¬¸ì„œ ê¸¸ì´: {total_chars:,} ë¬¸ì")
                logger.info(f"í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_chars:.0f} ë¬¸ì")
            
            logger.info("-" * 40)
            logger.info("ìƒì„±ëœ íŒŒì¼ë“¤:")
            
            if "comprehensive_markdown" in final_state:
                logger.info(f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {final_state['comprehensive_markdown']}")
            
            if "documents_pickle_path" in final_state:
                logger.info(f"Documents pickle: {final_state['documents_pickle_path']}")
                
            if "final_state_json_path" in final_state:
                logger.info(f"ìµœì¢… ìƒíƒœ JSON: {final_state['final_state_json_path']}")
                logger.info(f"ìµœì¢… ìƒíƒœ pickle: {final_state['final_state_pickle_path']}")
            
            if PSUTIL_AVAILABLE:
                try:
                    final_memory = psutil.virtual_memory()
                    logger.info(f"ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {final_memory.percent:.1f}%")
                except Exception:
                    pass
                
            logger.info("=" * 60)
        
        return final_state
        
    except Exception as e:
        total_time = time.time() - start_time
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ)")
        logger.error(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ ì§€ì : {type(e).__name__}")
        
        logger.debug("ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
        
        try:
            if 'final_state' in locals():
                logger.info("ë¶€ë¶„ì  ì²˜ë¦¬ ê²°ê³¼ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
                if "elements_from_parser" in final_state:
                    logger.info(f"ì²˜ë¦¬ëœ ìš”ì†Œ ìˆ˜: {len(final_state['elements_from_parser'])}")
        except Exception:
            pass
            
        raise 